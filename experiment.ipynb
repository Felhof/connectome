{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch.distributions\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from IPython.display import SVG\n",
    "\n",
    "import core\n",
    "from utils import sankey_diagram_of_connectome\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"attn-only-4l\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "prompt = \"When Mary and John went to the store, John gave a book to\"\n",
    "corrupt_prompt = \"When Tom and Sarah went to the store, Felix gave a book to\"\n",
    "c = core.connectom(model, prompt,\n",
    "                   core.logit_diff_metric(model, ' Mary', ' John'),\n",
    "                   core.ZeroPattern(),\n",
    "                   # core.CorruptIntervention(model, prompt, corrupt_prompt),\n",
    "                   # core.CropIntervention(model, prompt),\n",
    "                   # core.BasicStrategy(),\n",
    "                   # d.BacktrackBisectStrategy(threshold),\n",
    "                   # d.BacktrackingStrategy(threshold),\n",
    "                   # core.BisectStrategy(threshold),\n",
    "                   core.SplitStrategy(model, prompt, threshold),\n",
    "                   )\n",
    "core.plot_attn_connectome(model, prompt, c).show()\n",
    "# graph = core.plot_graphviz_connectome(model, prompt, c, threshold=threshold).pipe('svg').decode('utf-8')\n",
    "# SVG(graph)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = core.plot_graphviz_connectome(model, prompt, c, threshold=0.2).pipe('svg').decode('utf-8')\n",
    "SVG(graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"attn-only-4l\")\n",
    "threshold = 0.3\n",
    "prompt = '''def port(self, load, size, file, last):\n",
    "    \"\"\"oil column piece\n",
    "\n",
    "    :param load: crime population\n",
    "    :param size: unit dark\n",
    "    :param'''\n",
    "corrupt_prompt = prompt.replace(\"load\", \"banana\").replace(\"size\", \"apple\").replace(\"file\", \"pear\").replace(\"last\", \"orange\")\n",
    "\n",
    "c = core.connectom(model, prompt,\n",
    "                   core.logit_diff_metric(model, ' file', ' self', ' load', ' size', ' last'),\n",
    "                   core.ZeroPattern(),\n",
    "                   # core.CorruptIntervention(model, prompt, corrupt_prompt),\n",
    "\n",
    "                   # core.BasicStrategy(),\n",
    "                   core.SplitStrategy(model, prompt, threshold),\n",
    "                   # core.BacktrackBisectStrategy(threshold),\n",
    "                   # d.BacktrackingStrategy(threshold),\n",
    "                   # core.BisectStrategy(threshold),\n",
    "                   )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_threshold = 0.4\n",
    "core.plot_attn_connectome(model, prompt, c).show()\n",
    "graph = core.plot_graphviz_connectome(model, prompt, c, graph_threshold).pipe('svg').decode('utf-8')\n",
    "SVG(graph)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transformer_lens.utils.test_prompt(prompt, ' file', model)\n",
    "# logit, best = model(prompt)[0, -1].topk(10)\n",
    "# for b, l in zip(model.to_str_tokens(best), logit):\n",
    "#     print(b, l.item())\n",
    "print(prompt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = core.plot_graphviz_connectome(model, prompt, c, threshold=0.4).pipe('svg').decode('utf-8')\n",
    "SVG(graph)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.topk()\n",
    "dampening = 0.25\n",
    "c = core.connectom(model, prompt,\n",
    "                   core.logit_diff_metric(model, ' Mary', ' John'),\n",
    "                   # d.CorruptIntervention(model, prompt, corrupt_prompt),\n",
    "                   core.DampenIntervention(dampening),\n",
    "                   # d.ZeroPattern(),\n",
    "                   core.BasicStrategy(),\n",
    "                   )\n",
    "core.plot_attn_connectome(model, prompt, c).show()\n",
    "graph = core.plot_graphviz_connectome(model, prompt, c, threshold=0.05).pipe('svg').decode('utf-8')\n",
    "SVG(graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finding the log-probs on the prompt tokens\n",
    "prompt = \"When Mary and John went to the store, John gave a book to Mary.\"\n",
    "log_probs = model(prompt)[0].log_softmax(-1)  # (seq_len, vocab_size)\n",
    "tokens = model.to_tokens(prompt)[0]\n",
    "tokens_str = model.to_str_tokens(tokens)\n",
    "print(tokens.shape)\n",
    "print(log_probs.shape)\n",
    "correct_logprobs = log_probs[torch.arange(len(tokens) - 1), tokens[1:]]\n",
    "print(correct_logprobs.shape)\n",
    "for i, (t, n, l) in enumerate(zip(tokens_str, tokens_str[1:], correct_logprobs)):\n",
    "    print(f\"{i:2d} {t!r} {l.item():.2f} -> {n!r}\")\n",
    "\n",
    "import plotly.express as px\n",
    "px.line(x=[f'{i} {t!r} -> {n!r}' for i, (t, n) in enumerate(zip(tokens_str, tokens_str[1:]))],\n",
    "        y=correct_logprobs.detach() * 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = '''def port(self, load, size, file, last):\n",
    "    \"\"\"oil column piece\n",
    "\n",
    "    :param load: crime population\n",
    "    :param size: unit dark\n",
    "    :param'''\n",
    "\n",
    "print(prompt)\n",
    "s = core.SplitStrategy(model, prompt, 0.1,\n",
    "                       (\n",
    "                           '\\n\\n',\n",
    "                           tuple('.!?'),\n",
    "                           tuple(',:;'),\n",
    "                       )\n",
    "                       )\n",
    "s.show_tree()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
