{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch.distributions\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "from IPython.display import SVG\n",
    "\n",
    "import core\n",
    "from utils import sankey_diagram_of_connectome\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"attn-only-4l\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IOI task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "prompt = \"When Mary and John went to the store, John gave a book to\"\n",
    "corrupt_prompt = \"When Tom and Sarah went to the store, Felix gave a book to\"\n",
    "c = core.connectom(model, prompt,\n",
    "                   core.logit_diff_metric(model, ' Mary', ' John'),\n",
    "                   core.ZeroPattern(),\n",
    "                   # core.CorruptIntervention(model, prompt, corrupt_prompt),\n",
    "                   # core.CropIntervention(model, prompt),\n",
    "\n",
    "                   core.BasicStrategy(),\n",
    "                   # d.BacktrackBisectStrategy(threshold),\n",
    "                   # d.BacktrackingStrategy(threshold),\n",
    "                   # core.BisectStrategy(threshold),\n",
    "                   # core.SplitStrategy(model, prompt, threshold),\n",
    "                   )\n",
    "core.plot_attn_connectome(model, prompt, c).show()\n",
    "# graph = core.plot_graphviz_connectome(model, prompt, c, threshold=threshold).pipe('svg').decode('utf-8')\n",
    "# SVG(graph)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = core.plot_graphviz_connectome(model, prompt, c, threshold=0.2).pipe('svg').decode('utf-8')\n",
    "SVG(graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Docstring task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"attn-only-4l\")\n",
    "threshold = 0.3\n",
    "prompt = '''def port(self, load, size, file, last):\n",
    "    \"\"\"oil column piece\n",
    "\n",
    "    :param load: crime population\n",
    "    :param size: unit dark\n",
    "    :param'''\n",
    "corrupt_prompt = prompt.replace(\"load\", \"banana\").replace(\"size\", \"apple\").replace(\"file\", \"pear\").replace(\"last\", \"orange\")\n",
    "\n",
    "c = core.connectom(model, prompt,\n",
    "                   core.logit_diff_metric(model, ' file', ' self', ' load', ' size', ' last'),\n",
    "                   core.ZeroPattern(),\n",
    "                   # core.CorruptIntervention(model, prompt, corrupt_prompt),\n",
    "\n",
    "                   # core.BasicStrategy(),\n",
    "                   core.SplitStrategy(model, prompt, threshold),\n",
    "                   # core.BacktrackBisectStrategy(threshold),\n",
    "                   # d.BacktrackingStrategy(threshold),\n",
    "                   # core.BisectStrategy(threshold),\n",
    "                   )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_threshold = 0.4\n",
    "core.plot_attn_connectome(model, prompt, c).show()\n",
    "graph = core.plot_graphviz_connectome(model, prompt, c, graph_threshold).pipe('svg').decode('utf-8')\n",
    "SVG(graph)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(prompt)\n",
    "transformer_lens.utils.test_prompt(prompt, ' file', model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code task on Pythia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc5c7ef2de1f4083b7a3c00506e92d53"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5788391e2f364325a1066b880582e223"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "319b2046fe7248979cb4a409a87f7daa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0371ed2eda5e4010b833c03d504f7301"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aea984ae8f2d4498a5da389bec664220"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbd730579f3b4668ae838d4bb0591ce8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-medium into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained('gpt2-medium')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T22:48:27.740956093Z",
     "start_time": "2023-06-27T22:43:19.591141207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'from', ' typing', ' import', ' List', ',', ' D', 'ict', '\\n', 'def', ' f', '(', 'x', ':', ' List', '):', '\\n', ' ', ' ', ' ', ' return', ' sum', '(', 'x', '[', '::', '2', '])', '\\n', '\\n', 'def', ' g', '(', 'x', ':', ' float', '):', '\\n', ' ', ' ', ' ', ' return', ' x', ' **', ' 2', '\\n', '\\n', 'def', ' h', '(', 'x', ':', ' D', 'ict', '):', '\\n', ' ', ' ', ' ', ' return', ' sum', '(', 'x', '.', 'values', '())', '\\n', '\\n', 'def', ' i', '(', 'x', ':', ' str', '):', '\\n', ' ', ' ', ' ', ' return', ' len', '(', 'x', ')', '\\n', '\\n', 'var', '1', ':', ' str', ' =', \" '\", 'abc', \"'\", '\\n', 'var', '2', ':', ' D', 'ict', ' =', ' {', \"'\", 'a', \"':\", ' 1', ',', \" '\", 'b', \"':\", ' 2', '}', '\\n', 'var', '3', ':', ' List', ' =', ' [', '1', ',', ' 2', ',', ' 3', ']', '\\n', 'var', '4', ':', ' int', ' =', ' 4', '\\n', '\\n', 'h', '(', 'var']\n",
      "Tokenized answer: [' 1']\n"
     ]
    },
    {
     "data": {
      "text/plain": "Performance on answer token:\n\u001B[1mRank: \u001B[0m\u001B[1;36m20\u001B[0m\u001B[1m       Logit: \u001B[0m\u001B[1;36m11.51\u001B[0m\u001B[1m Prob:  \u001B[0m\u001B[1;36m0.03\u001B[0m\u001B[1m% Token: | \u001B[0m\u001B[1;36m1\u001B[0m\u001B[1m|\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11.51</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">|</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 19.21 Prob: 70.42% Token: |1|\n",
      "Top 1th token. Logit: 17.25 Prob:  9.88% Token: |3|\n",
      "Top 2th token. Logit: 17.06 Prob:  8.19% Token: |4|\n",
      "Top 3th token. Logit: 16.55 Prob:  4.91% Token: |2|\n",
      "Top 4th token. Logit: 16.18 Prob:  3.39% Token: |5|\n",
      "Top 5th token. Logit: 14.41 Prob:  0.58% Token: |0|\n",
      "Top 6th token. Logit: 14.15 Prob:  0.45% Token: |,|\n",
      "Top 7th token. Logit: 13.39 Prob:  0.21% Token: |:|\n",
      "Top 8th token. Logit: 13.23 Prob:  0.18% Token: |)|\n",
      "Top 9th token. Logit: 13.06 Prob:  0.15% Token: |.|\n"
     ]
    },
    {
     "data": {
      "text/plain": "\u001B[1mRanks of the answer tokens:\u001B[0m \u001B[1m[\u001B[0m\u001B[1m(\u001B[0m\u001B[32m' 1'\u001B[0m, \u001B[1;36m20\u001B[0m\u001B[1m)\u001B[0m\u001B[1m]\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 1'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"font-weight: bold\">)]</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = \"\"\"from typing import List, Dict\n",
    "def f(x: List):\n",
    "    return sum(x[::2])\n",
    "\n",
    "def g(x: float):\n",
    "    return x ** 2\n",
    "\n",
    "def h(x: Dict):\n",
    "    return sum(x.values())\n",
    "\n",
    "def i(x: str):\n",
    "    return len(x)\n",
    "\n",
    "var1: str = 'abc'\n",
    "var2: Dict = {'a': 1, 'b': 2}\n",
    "var3: List = [1, 2, 3]\n",
    "var4: int = 4\n",
    "\n",
    "h(var\"\"\"\n",
    "\n",
    "transformer_lens.utils.test_prompt(prompt, '1', model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-27T22:42:53.723206939Z",
     "start_time": "2023-06-27T22:42:53.341737228Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploration of grouping techniques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finding the log-probs on the prompt tokens\n",
    "prompt = \"When Mary and John went to the store, John gave a book to Mary.\"\n",
    "log_probs = model(prompt)[0].log_softmax(-1)  # (seq_len, vocab_size)\n",
    "tokens = model.to_tokens(prompt)[0]\n",
    "tokens_str = model.to_str_tokens(tokens)\n",
    "print(tokens.shape)\n",
    "print(log_probs.shape)\n",
    "correct_logprobs = log_probs[torch.arange(len(tokens) - 1), tokens[1:]]\n",
    "print(correct_logprobs.shape)\n",
    "for i, (t, n, l) in enumerate(zip(tokens_str, tokens_str[1:], correct_logprobs)):\n",
    "    print(f\"{i:2d} {t!r} {l.item():.2f} -> {n!r}\")\n",
    "\n",
    "import plotly.express as px\n",
    "px.line(x=[f'{i} {t!r} -> {n!r}' for i, (t, n) in enumerate(zip(tokens_str, tokens_str[1:]))],\n",
    "        y=correct_logprobs.detach() * 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = '''def port(self, load, size, file, last):\n",
    "    \"\"\"oil column piece\n",
    "\n",
    "    :param load: crime population\n",
    "    :param size: unit dark\n",
    "    :param'''\n",
    "\n",
    "print(prompt)\n",
    "s = core.SplitStrategy(model, prompt, 0.1,\n",
    "                       (\n",
    "                           '\\n\\n',\n",
    "                           tuple('.!?'),\n",
    "                           tuple(',:;'),\n",
    "                       )\n",
    "                       )\n",
    "s.show_tree()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
