{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import torch.distributions\n",
    "import transformer_lens\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "import connectome as core\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attn_only_4l = HookedTransformer.from_pretrained(\"attn-only-4l\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# IOI task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "prompt = \"When Mary and John went to the store, John gave a book to\"\n",
    "corrupt_prompt = \"When Tom and Sarah went to the store, Felix gave a book to\"\n",
    "metric = core.logit_diff_metric(model, \" Mary\", \" John\")\n",
    "c = core.connectome(\n",
    "    model,\n",
    "    prompt,\n",
    "    metric,\n",
    "    core.ZeroPattern(),\n",
    "    # core.CorruptIntervention(model, prompt, corrupt_prompt),\n",
    "    # core.CropIntervention(model, prompt),\n",
    "    core.BasicStrategy(),\n",
    "    # d.BacktrackBisectStrategy(threshold),\n",
    "    # d.BacktrackingStrategy(threshold),\n",
    "    # core.BisectStrategy(threshold),\n",
    "    # core.SplitStrategy(model, prompt, threshold, delimiters_as_leaves=True),\n",
    ")\n",
    "core.plot_attn_connectome(model, prompt, c).show()\n",
    "# graph = core.plot_graphviz_connectome(model, prompt, c, threshold=threshold).pipe('svg').decode('utf-8')\n",
    "# SVG(graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph = core.plot_graphviz_connectome(model, prompt, c, depth=2, top_k=15)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted_connectome = sorted(c, key=lambda x: abs(x.strength), reverse=True)\n",
    "thresholds = torch.linspace(0, 1.0, 20)\n",
    "top_ks = list(range(1, len(c)))\n",
    "for dampen_weak in [0, 0.2, 0.4, 0.6]:\n",
    "    strength_kept = [\n",
    "        core.cut_connectome(\n",
    "            model,\n",
    "            prompt,\n",
    "            metric,\n",
    "            core.filter_connectome(c, None, top_k=top_k),\n",
    "            dampen_weak=dampen_weak,\n",
    "        )\n",
    "        for top_k in top_ks\n",
    "        # for threshold in thresholds\n",
    "    ]\n",
    "    px.line(\n",
    "        x=top_ks,\n",
    "        y=strength_kept,\n",
    "        title=f\"Strength kept when keeping top connections and dampening other by {dampen_weak:.1f}\",\n",
    "        labels={\"x\": \"Top k\", \"y\": \"Strength kept\"},\n",
    "        width=800,\n",
    "    ).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "px.line(x=top_ks, y=strength_kept)\n",
    "# px.line(x=thresholds, y=strength_kept)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Docstring task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"attn-only-4l\")\n",
    "threshold = 0.3\n",
    "prompt = '''def port(self, load, size, file, last):\n",
    "    \"\"\"oil column piece\n",
    "\n",
    "    :param load: crime population\n",
    "    :param size: unit dark\n",
    "    :param'''\n",
    "corrupt_prompt = (\n",
    "    prompt.replace(\"load\", \"banana\")\n",
    "    .replace(\"size\", \"apple\")\n",
    "    .replace(\"file\", \"pear\")\n",
    "    .replace(\"last\", \"orange\")\n",
    ")\n",
    "\n",
    "c = core.connectom(\n",
    "    model,\n",
    "    prompt,\n",
    "    core.logit_diff_metric(model, \" file\", \" self\", \" load\", \" size\", \" last\"),\n",
    "    core.ZeroPattern(),\n",
    "    # core.CorruptIntervention(model, prompt, corrupt_prompt),\n",
    "    # core.BasicStrategy(),\n",
    "    core.SplitStrategy(model, prompt, threshold, delimiters_as_leaves=True),\n",
    "    # core.BacktrackBisectStrategy(threshold),\n",
    "    # d.BacktrackingStrategy(threshold),\n",
    "    # core.BisectStrategy(threshold),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_threshold = 0.4\n",
    "core.plot_attn_connectome(model, prompt, c).show()\n",
    "for depth in [2, 3, 4]:\n",
    "    graph = core.graphviz_connectome(model, prompt, c, graph_threshold, depth=depth)\n",
    "    svg = graph.pipe(\"svg\").decode(\"utf-8\")\n",
    "    display(graph)\n",
    "    import datetime\n",
    "\n",
    "    date = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "    with open(f\"graphs/graph-{date}.svg\", \"w\") as f:\n",
    "        f.write(svg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(prompt)\n",
    "transformer_lens.utils.test_prompt(prompt, \" file\", model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Code task on Pythia"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-medium\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"\"\"from typing import List, Dict\n",
    "def f(x: List):\n",
    "    return sum(x[::2])\n",
    "\n",
    "def g(x: float):\n",
    "    return x ** 2\n",
    "\n",
    "def h(x: Dict):\n",
    "    return sum(x.values())\n",
    "\n",
    "def i(x: str):\n",
    "    return len(x)\n",
    "\n",
    "var1: str = 'abc'\n",
    "var2: Dict = {'a': 1, 'b': 2}\n",
    "var3: List = [1, 2, 3]\n",
    "var4: int = 4\n",
    "\n",
    "h(var\"\"\"\n",
    "\n",
    "transformer_lens.utils.test_prompt(prompt, \"1\", model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploration of grouping techniques"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Finding the log-probs on the prompt tokens\n",
    "prompt = \"When Mary and John went to the store, John gave a book to Mary.\"\n",
    "log_probs = model(prompt)[0].log_softmax(-1)  # (seq_len, vocab_size)\n",
    "tokens = model.to_tokens(prompt)[0]\n",
    "tokens_str = model.to_str_tokens(tokens)\n",
    "print(tokens.shape)\n",
    "print(log_probs.shape)\n",
    "correct_logprobs = log_probs[torch.arange(len(tokens) - 1), tokens[1:]]\n",
    "print(correct_logprobs.shape)\n",
    "for i, (t, n, l) in enumerate(zip(tokens_str, tokens_str[1:], correct_logprobs)):\n",
    "    print(f\"{i:2d} {t!r} {l.item():.2f} -> {n!r}\")\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "px.line(\n",
    "    x=[\n",
    "        f\"{i} {t!r} -> {n!r}\"\n",
    "        for i, (t, n) in enumerate(zip(tokens_str, tokens_str[1:]))\n",
    "    ],\n",
    "    y=correct_logprobs.detach() * 0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = '''def port(self, load, size, file, last):\n",
    "    \"\"\"oil column piece\n",
    "\n",
    "    :param load: crime population\n",
    "    :param size: unit dark\n",
    "    :param'''\n",
    "\n",
    "print(prompt)\n",
    "s = core.SplitStrategy(\n",
    "    model,\n",
    "    prompt,\n",
    "    0.1,\n",
    "    (\n",
    "        \"\\n\\n\",\n",
    "        tuple(\".!?\"),\n",
    "        tuple(\",:;\"),\n",
    "    ),\n",
    ")\n",
    "s.show_tree()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(core.filter_connectome(c, 1))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(c)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pythia CODE task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "from typing import List\n",
    "from math import pi\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, x: float, y: float) -> None:\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "class A:\n",
    "    def __init__(self, bottom_left: Point, top_right: Point) -> None:\n",
    "        self.bottom_left = bottom_left\n",
    "        self.top_right = top_right\n",
    "\n",
    "class B:\n",
    "    def __init__(self, center: Point, radius: float) -> None:\n",
    "        self.center = center\n",
    "        self.radius = radius\n",
    "\n",
    "class C:\n",
    "    def __init__(self, points: List[Point]) -> None:\n",
    "        self.points = points\n",
    "\n",
    "def calculate_area(rectangle: A) -> float:\n",
    "    height = rectangle.top_right.y - rectangle.bottom_left.y\n",
    "    width = rectangle.top_right.x - rectangle.bottom_left.x\n",
    "    return height * width\n",
    "\n",
    "def calculate_center(rectangle: A) -> Point:\n",
    "    center_x = (rectangle.bottom_left.x + rectangle.top_right.x) / 2\n",
    "    center_y = (rectangle.bottom_left.y + rectangle.top_right.y) / 2\n",
    "    return Point(center_x, center_y)\n",
    "\n",
    "def calculate_distance(point1: Point, point2: Point) -> float:\n",
    "    return ((point2.x - point1.x) ** 2 + (point2.y - point1.y) ** 2) ** 0.5\n",
    "\n",
    "def calculate_circumference(circle: B) -> float:\n",
    "    return 2 * pi * circle.radius\n",
    "\n",
    "def calculate_circle_area(circle: B) -> float:\n",
    "    return pi * (circle.radius ** 2)\n",
    "\n",
    "def calculate_perimeter(polygon: C) -> float:\n",
    "    perimeter = 0\n",
    "    points = polygon.points + [polygon.points[0]]  # Add the first point at the end for a closed shape\n",
    "    for i in range(len(points) - 1):\n",
    "        perimeter += calculate_distance(points[i], points[i + 1])\n",
    "    return perimeter\n",
    "\n",
    "foo = A(Point(2, 3), Point(6, 5))\n",
    "\n",
    "bar = B(Point(0, 0), 5)\n",
    "\n",
    "name = C([Point(0, 0), Point(1, 0), Point(0, 1)])\n",
    "\n",
    "# Calculate circumference\n",
    "print(calculate_circumference(\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import GPTNeoXTokenizerFast\n",
    "\n",
    "tokenizer: GPTNeoXTokenizerFast = AutoTokenizer.from_pretrained(\n",
    "    \"EleutherAI/pythia-2.8b\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens = [\"|BOS|\"] + tokenizer.batch_decode(tokenizer(prompt)[\"input_ids\"])\n",
    "labels = [f\"{i} {t!r}\" for i, t in enumerate(tokens)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_attention = torch.load(\"avg_attention.pt\", map_location=\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.imshow(\n",
    "    max_attention,\n",
    "    x=labels,\n",
    "    y=labels,\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    height=6000,\n",
    "    width=6000,\n",
    "    title=\"Max attention matrix for Pythia code task\",\n",
    "    labels=dict(x=\"Source\", y=\"Target\"),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
